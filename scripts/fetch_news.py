#!/usr/bin/env python3
"""
fetch_news.py â€” Naver News Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ ì „ì¼ì ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

í™˜ê²½ ë³€ìˆ˜:
  NAVER_CLIENT_ID     â€” Naver ê°œë°œì Client ID
  NAVER_CLIENT_SECRET â€” Naver ê°œë°œì Client Secret
"""

import json
import os
import re
import sys
from datetime import datetime
from datetime import timedelta
from urllib.parse import quote

import requests

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORIES = [
    {
        "label": "ğŸŒ ê±°ì‹œê²½ì œ & ê¸ˆìœµ",
        "queries": ["ì½”ìŠ¤í”¼ ë§ˆê° ì‹œí™©", "ê¸ˆê°ì› ê¸ˆìœµ ì œì¬", "í•œêµ­ì€í–‰ ê¸ˆë¦¬", "ì›ë‹¬ëŸ¬ í™˜ìœ¨"],
        "max_articles": 3,
    },
    {
        "label": "ğŸ’» ë°˜ë„ì²´/IT",
        "queries": ["ì‚¼ì„±ì „ì ë°˜ë„ì²´", "SKí•˜ì´ë‹‰ìŠ¤ HBM", "ë°˜ë„ì²´ ìˆ˜ì¶œ ì‹¤ì "],
        "max_articles": 3,
    },
    {
        "label": "ğŸ”‹ 2ì°¨ì „ì§€/ì—ë„ˆì§€",
        "queries": ["2ì°¨ì „ì§€ ë°°í„°ë¦¬ ìˆ˜ì£¼", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "SKì˜¨ ESS"],
        "max_articles": 3,
    },
    {
        "label": "ğŸ›¡ï¸ ê¸ˆìœµ/ë°°ë‹¹/ë°©ì–´ì£¼",
        "queries": ["ê³ ë°°ë‹¹ì£¼ ETF", "KBê¸ˆìœµ ë°°ë‹¹", "ì£¼ì£¼í™˜ì› ìì‚¬ì£¼"],
        "max_articles": 3,
    },
]

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")


def get_yesterday() -> str:
    """ì–´ì œ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")


def search_naver_news(query: str, client_id: str, client_secret: str, display: int = 10) -> list[dict]:
    """Naver ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": display,
        "sort": "date",
    }
    resp = requests.get(url, headers=headers, params=params, timeout=10)
    resp.raise_for_status()
    return resp.json().get("items", [])


def clean_html(text: str) -> str:
    """Naver API ì‘ë‹µì˜ HTML íƒœê·¸(<b> ë“±)ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return re.sub(r"<[^>]+>", "", text).strip()


def extract_original_link(item: dict) -> str:
    """ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ì•ˆì •ì„±ì„ ìœ„í•´ news.naver.com ë§í¬ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤."""
    link = item.get("link", "")
    originallink = item.get("originallink", "")

    # news.naver.com ë§í¬ê°€ ìˆìœ¼ë©´ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆìŒ (ë„¤ì´ë²„ ë‰´ìŠ¤ íŒ)
    if "news.naver.com" in link:
        return link
    
    # ì—†ìœ¼ë©´ ì–¸ë¡ ì‚¬ ì›ë¬¸ ë§í¬ ì‚¬ìš©
    return originallink or link


def is_yesterday(link: str, pub_date_str: str, yesterday: str) -> bool:
    """ê¸°ì‚¬ê°€ ì „ì¼ìì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤ (URL íŒ¨í„´ + pubDate)."""
    ymd = yesterday.replace("-", "")  # 20260216
    y_dot = yesterday.replace("-", ".")  # 2026.02.16

    # URLì— ë‚ ì§œ í¬í•¨ ì—¬ë¶€
    if ymd in link or y_dot in link or yesterday in link:
        return True

    # pubDate íŒŒì‹± (ì˜ˆ: "Mon, 16 Feb 2026 09:00:00 +0900")
    try:
        pub_dt = datetime.strptime(pub_date_str.strip(), "%a, %d %b %Y %H:%M:%S %z")
        if pub_dt.strftime("%Y-%m-%d") == yesterday:
            return True
    except (ValueError, TypeError):
        pass

    return False


def fetch_all(client_id: str, client_secret: str) -> dict:
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    yesterday = get_yesterday()
    print(f"[fetch_news] ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {yesterday}")

    result = {"date": yesterday, "categories": []}

    for cat in CATEGORIES:
        articles = []
        seen_urls = set()

        for query in cat["queries"]:
            try:
                items = search_naver_news(query, client_id, client_secret)
            except Exception as e:
                print(f"  âš  ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}")
                continue

            for item in items:
                link = extract_original_link(item)
                if not link or link in seen_urls:
                    continue

                if not is_yesterday(link, item.get("pubDate", ""), yesterday):
                    continue

                title = clean_html(item.get("title", ""))
                if not title:
                    continue

                seen_urls.add(link)
                articles.append({"title": title, "url": link})

                if len(articles) >= cat["max_articles"]:
                    break

            if len(articles) >= cat["max_articles"]:
                break

        print(f"  âœ“ {cat['label']}: {len(articles)}ê±´ ìˆ˜ì§‘")
        result["categories"].append({
            "label": cat["label"],
            "articles": articles,
        })

    return result


def main():
    client_id = os.environ.get("X_NAVER_CLIENT_ID", "")
    client_secret = os.environ.get("X_NAVER_CLIENT_SECRET", "")

    print(f"DEBUG: Checking environment variables...")
    print(f"DEBUG: X_NAVER_CLIENT_ID present: {bool(client_id)}")
    print(f"DEBUG: X_NAVER_CLIENT_SECRET present: {bool(client_secret)}")
    
    if client_id:
        print(f"DEBUG: X_NAVER_CLIENT_ID length: {len(client_id)}")
        print(f"DEBUG: X_NAVER_CLIENT_ID starts with: {client_id[:2]}***")

    if not client_id or not client_secret:
        print("âŒ X_NAVER_CLIENT_ID / X_NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ GitHub Repo > Settings > Secrets and variables > Actions > Repository secrets ì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    data = fetch_all(client_id, client_secret)

    os.makedirs(DATA_DIR, exist_ok=True)
    out_path = os.path.join(DATA_DIR, "articles.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    total = sum(len(c["articles"]) for c in data["categories"])
    print(f"\nâœ… ì´ {total}ê±´ ì €ì¥ â†’ {out_path}")


if __name__ == "__main__":
    main()
