#!/usr/bin/env python3
"""
fetch_news.py â€” Naver News Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ ì „ì¼ì ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

í™˜ê²½ ë³€ìˆ˜:
  NAVER_CLIENT_ID     â€” Naver ê°œë°œì Client ID
  NAVER_CLIENT_SECRET â€” Naver ê°œë°œì Client Secret
"""

import json
import os
import re
import sys
from datetime import datetime
from datetime import timedelta
from urllib.parse import quote

import requests

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORIES = [
    {
        "label": "ğŸŒ ê±°ì‹œê²½ì œ & ê¸ˆìœµ",
        "queries": ["ì½”ìŠ¤í”¼ ë§ˆê° ì‹œí™©", "ê¸ˆê°ì› ê¸ˆìœµ ì œì¬", "í•œêµ­ì€í–‰ ê¸ˆë¦¬", "ì›ë‹¬ëŸ¬ í™˜ìœ¨"],
        "max_articles": 3,
    },
    {
        "label": "ğŸ’» ë°˜ë„ì²´/IT",
        "queries": ["ì‚¼ì„±ì „ì ë°˜ë„ì²´", "SKí•˜ì´ë‹‰ìŠ¤ HBM", "ë°˜ë„ì²´ ìˆ˜ì¶œ ì‹¤ì "],
        "max_articles": 3,
    },
    {
        "label": "ğŸ”‹ 2ì°¨ì „ì§€/ì—ë„ˆì§€",
        "queries": ["2ì°¨ì „ì§€ ë°°í„°ë¦¬ ìˆ˜ì£¼", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "SKì˜¨ ESS"],
        "max_articles": 3,
    },
    {
        "label": "ğŸ›¡ï¸ ê¸ˆìœµ/ë°°ë‹¹/ë°©ì–´ì£¼",
        "queries": ["ê³ ë°°ë‹¹ì£¼ ETF", "KBê¸ˆìœµ ë°°ë‹¹", "ì£¼ì£¼í™˜ì› ìì‚¬ì£¼"],
        "max_articles": 3,
    },
]

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")


def get_yesterday() -> str:
    """ì–´ì œ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")


def search_naver_news(query: str, client_id: str, client_secret: str, display: int = 10) -> list[dict]:
    """Naver ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    params = {
        "query": query,
        "display": display,
        "sort": "date",
    }
    resp = requests.get(url, headers=headers, params=params, timeout=10)
    resp.raise_for_status()
    return resp.json().get("items", [])


def clean_html(text: str) -> str:
    """Naver API ì‘ë‹µì˜ HTML íƒœê·¸(<b> ë“±)ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return re.sub(r"<[^>]+>", "", text).strip()


def is_yesterday(link: str, pub_date_str: str, yesterday: str) -> bool:
    """ê¸°ì‚¬ê°€ ì „ì¼ìì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤ (URL íŒ¨í„´ + pubDate)."""
    ymd = yesterday.replace("-", "")  # 20260216
    y_dot = yesterday.replace("-", ".")  # 2026.02.16

    # URLì— ë‚ ì§œ í¬í•¨ ì—¬ë¶€
    if ymd in link or y_dot in link or yesterday in link:
        return True

    # pubDate íŒŒì‹± (ì˜ˆ: "Mon, 16 Feb 2026 09:00:00 +0900")
    try:
        pub_dt = datetime.strptime(pub_date_str.strip(), "%a, %d %b %Y %H:%M:%S %z")
        if pub_dt.strftime("%Y-%m-%d") == yesterday:
            return True
    except (ValueError, TypeError):
        pass

    return False


def fetch_all(client_id: str, client_secret: str) -> dict:
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    yesterday = get_yesterday()
    print(f"[fetch_news] ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {yesterday}")

    result = {"date": yesterday, "categories": []}

    for cat in CATEGORIES:
        articles = []
        seen_links = set()

        for query in cat["queries"]:
            try:
                items = search_naver_news(query, client_id, client_secret)
            except Exception as e:
                print(f"  âš  ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}")
                continue

            # Process items from the search result
            filtered_items = []
            for item in items:
                title = clean_html(item.get("title", ""))
                if not title:
                    continue

                # ğŸš€ [ë³€ê²½] ë„¤ì´ë²„ ë‰´ìŠ¤(news.naver.com) ë§í¬ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
                # ì™¸ë¶€ ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸ëŠ” ë§í¬ê°€ ì£½ê±°ë‚˜(404), ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ì œì™¸í•©ë‹ˆë‹¤.
                link = item.get("link", "")
                if "news.naver.com" not in link:
                    continue

                # ë‚ ì§œ í•„í„°ë§ (ì „ì¼ì)
                pub_date_str = item.get("pubDate", "")
                if not is_yesterday(link, pub_date_str, yesterday): # Pass link to is_yesterday
                    continue
                
                # ì¤‘ë³µ ì œê±°
                if link in seen_links:
                    continue
                seen_links.add(link)

                filtered_items.append({
                    "title": title,
                    "url": link,  # news.naver.com ë§í¬ (changed from 'link' to 'url' to match original structure)
                    # "pubDate": pub_date_str, # Not needed in final articles list
                    # "description": clean_html(item.get("description", "")) # Not needed in final articles list
                })
            
            # Add filtered items to articles, respecting max_articles
            for article_item in filtered_items:
                if len(articles) >= cat["max_articles"]:
                    break
                articles.append(article_item)

            if len(articles) >= cat["max_articles"]:
                break

        print(f"  âœ“ {cat['label']}: {len(articles)}ê±´ ìˆ˜ì§‘")
        result["categories"].append({
            "label": cat["label"],
            "articles": articles,
        })

    return result


def main():
    client_id = os.environ.get("X_NAVER_CLIENT_ID", "")
    client_secret = os.environ.get("X_NAVER_CLIENT_SECRET", "")

    if not client_id or not client_secret:
        print("âŒ X_NAVER_CLIENT_ID / X_NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    data = fetch_all(client_id, client_secret)

    os.makedirs(DATA_DIR, exist_ok=True)
    out_path = os.path.join(DATA_DIR, "articles.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    total = sum(len(c["articles"]) for c in data["categories"])
    print(f"\nâœ… ì´ {total}ê±´ ì €ì¥ â†’ {out_path}")


if __name__ == "__main__":
    main()
